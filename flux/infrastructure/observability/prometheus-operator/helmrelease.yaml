apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
    name: prometheus
    namespace: observability
spec:
    interval: 30m
    chart:
        spec:
            chart: kube-prometheus-stack
            version: "81.3.0"
            sourceRef:
                kind: HelmRepository
                name: prometheus-community
                namespace: observability
            interval: 12h
    install:
        crds: CreateReplace
        remediation:
            retries: 3
    upgrade:
        crds: CreateReplace
        remediation:
            retries: 3
    values:
        # TODO: configure gateway
        prometheus:
            service:
                annotations:
                    liqo.io/allow-reflection: "true"

            prometheusSpec:
                # Thanos handles incoming metrics from Chaos and long term storage on S3
                thanos:
                    image: quay.io/thanos/thanos:v0.39.2
                    objectStorageConfig:
                        key: thanos.yaml
                        name: axiom-thanos-objstore-config

                    arguments:
                        - name: objstore.config-file
                          value: /secrets/axiom-thanos-objstore-config/objstore.yml

                    volumeMounts:
                        - name: axiom-thanos-objstore-config
                          mountPath: /secrets/axiom-thanos-objstore-config
                          readOnly: true

                volumes:
                    - name: axiom-thanos-objstore-config
                      secret:
                          secretName: axiom-thanos-objstore-config

                externalLabels:
                    cluster: axiom

                storageSpec:
                    # preserve some data in Axiom's Ceph so that it isn't lost upon a restart
                    # volumeClaimTemplate:
                    #     spec:
                    #         storageClassName: rook-ceph-block
                    #         accessModes: ["ReadWriteOnce"]
                    #         resources:
                    #             requests:
                    #                 storage: 5Gi

                retention: 24h

                serviceMonitorSelector: {}
                serviceMonitorSelectorNilUsesHelmValues: false
                podMonitorSelector: {}
                podMonitorSelectorNilUsesHelmValues: false
                additionalScrapeConfigs:
                    - job_name: "zfs-exporter"
                      static_configs:
                          - targets:
                                - "10.67.1.1:9134"
                                - "192.168.2.40:9134"
                      metrics_path: /metrics
                      scrape_interval: 30s

                    - job_name: "zrepl"
                      static_configs:
                          - targets:
                                - "192.168.2.40:9811"
                      metrics_path: /metrics
                      scrape_interval: 30s

                    # used for Cilium
                    - job_name: "kubernetes-pods"
                      kubernetes_sd_configs:
                          - role: pod
                      relabel_configs:
                          - source_labels: [__meta_kubernetes_namespace]
                            action: keep
                            regex: kube-system
                          - source_labels:
                                [
                                    __meta_kubernetes_pod_annotation_prometheus_io_scrape,
                                ]
                            action: keep
                            regex: true
                          - source_labels:
                                [
                                    __address__,
                                    __meta_kubernetes_pod_annotation_prometheus_io_port,
                                ]
                            action: replace
                            regex: ([^:]+)(?::\d+)?;(\d+)
                            replacement: ${1}:${2}
                            target_label: __address__

                    # used for Cilium Hubble
                    - job_name: "kubernetes-endpoints"
                      scrape_interval: 30s
                      kubernetes_sd_configs:
                          - role: endpoints
                      relabel_configs:
                          - source_labels: [__meta_kubernetes_namespace]
                            action: keep
                            regex: kube-system
                          - source_labels:
                                [
                                    __meta_kubernetes_service_annotation_prometheus_io_scrape,
                                ]
                            action: keep
                            regex: true
                          - source_labels:
                                [
                                    __address__,
                                    __meta_kubernetes_service_annotation_prometheus_io_port,
                                ]
                            action: replace
                            target_label: __address__
                            regex: (.+)(?::\d+);(\d+)
                            replacement: $1:$2

            thanosService:
                enabled: true
            thanosServiceMonitor:
                enabled: true

        grafana:
            additionalDataSources:
                - name: Thanos
                  type: prometheus
                  url: http://thanos-query.observability.svc.cluster.local:9090
                  access: proxy
                  uid: axiom-thanos
                  isDefault: true

            sidecar:
                dashboards:
                    multicluster:
                        global:
                            enabled: true
                        etcd:
                            enabled: true

                datasources:
                    # Prometheus directly is not the default anymore, instead query Thanos
                    isDefaultDatasource: false

            dashboardProviders:
                dashboardproviders.yaml:
                    apiVersion: 1
                    providers:
                        - name: "rookceph"
                          orgId: 1
                          folder: "Rook Ceph"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/rookceph
                        - name: "zfs"
                          orgId: 1
                          folder: "ZFS"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/zfs
                        - name: "cilium"
                          orgId: 1
                          folder: "Cilium"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/cilium
                        - name: "cloudnative-pg"
                          orgId: 1
                          folder: "CloudNativePG"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/cloudnative-pg
                        - name: "traefik"
                          orgId: 1
                          folder: "Traefik"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/traefik
                        - name: "proxmox"
                          orgId: 1
                          folder: "Proxmox"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/proxmox
                        - name: "external-dns"
                          orgId: 1
                          folder: "External DNS"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/external-dns
                        - name: "liqo"
                          orgId: 1
                          folder: "Liqo"
                          type: file
                          disableDeletion: false
                          editable: true
                          options:
                              path: /var/lib/grafana/dashboards/liqo
            dashboards:
                rookceph:
                    cluster-dashboard:
                        url: https://raw.githubusercontent.com/rook/rook/refs/heads/master/deploy/examples/monitoring/grafana/Ceph%20Cluster%20Dashboard.json
                    osd-single-dashboard:
                        url: https://raw.githubusercontent.com/rook/rook/refs/heads/master/deploy/examples/monitoring/grafana/Ceph%20OSD%20Single%20Dashboard.json
                    pools-dashboard:
                        url: https://raw.githubusercontent.com/rook/rook/refs/heads/master/deploy/examples/monitoring/grafana/Ceph%20Pools%20Dashboard.json
                zfs:
                    arc-stats:
                        url: https://raw.githubusercontent.com/d-uzlov/k8s-homelab/refs/heads/master/metrics/zfs-exporter/dashboards/zfs-arc-stats.json
                    dataset-details:
                        url: https://raw.githubusercontent.com/d-uzlov/k8s-homelab/refs/heads/master/metrics/zfs-exporter/dashboards/zfs-dataset-details.json
                    pool-datasets:
                        url: https://raw.githubusercontent.com/d-uzlov/k8s-homelab/refs/heads/master/metrics/zfs-exporter/dashboards/zfs-pool-datasets.json
                    pool-overview:
                        url: https://raw.githubusercontent.com/d-uzlov/k8s-homelab/refs/heads/master/metrics/zfs-exporter/dashboards/zfs-pool-overview.json
                    vdev-disks:
                        url: https://raw.githubusercontent.com/d-uzlov/k8s-homelab/refs/heads/master/metrics/zfs-exporter/dashboards/zfs-vdev-disks.json
                    vdev-list:
                        url: https://raw.githubusercontent.com/d-uzlov/k8s-homelab/refs/heads/master/metrics/zfs-exporter/dashboards/zfs-vdev-list.json
                    zrepl:
                        url: https://raw.githubusercontent.com/zrepl/zrepl/refs/heads/master/dist/grafana/grafana-prometheus-zrepl.json
                        datasource:
                            - name: DS_PROMETHEUS
                              value: "Thanos"
                cilium:
                    agent:
                        url: https://raw.githubusercontent.com/cilium/cilium/refs/heads/main/install/kubernetes/cilium/files/cilium-agent/dashboards/cilium-dashboard.json
                    hubble:
                        url: https://raw.githubusercontent.com/cilium/cilium/refs/heads/main/install/kubernetes/cilium/files/hubble/dashboards/hubble-dashboard.json
                    hubble-dns-namespace:
                        url: https://raw.githubusercontent.com/cilium/cilium/refs/heads/main/install/kubernetes/cilium/files/hubble/dashboards/hubble-dns-namespace.json
                    hubble-l7-http-metrics-by-workload:
                        url: https://raw.githubusercontent.com/cilium/cilium/refs/heads/main/install/kubernetes/cilium/files/hubble/dashboards/hubble-l7-http-metrics-by-workload.json
                    hubble-network-overview-namespace:
                        url: https://raw.githubusercontent.com/cilium/cilium/refs/heads/main/install/kubernetes/cilium/files/hubble/dashboards/hubble-network-overview-namespace.json
                cloudnative-pg:
                    dashboard:
                        url: https://raw.githubusercontent.com/cloudnative-pg/grafana-dashboards/refs/heads/main/charts/cluster/grafana-dashboard.json
                liqo:
                    network:
                        url: https://raw.githubusercontent.com/liqotech/liqo/refs/heads/master/docs/_downloads/grafana/liqonetwork.json
                    kubelet:
                        url: https://raw.githubusercontent.com/liqotech/liqo/refs/heads/master/docs/_downloads/grafana/liqovirtualkubelet.json
                traefik:
                    traefik-official:
                        gnetId: 17347
                        revision: 9
                        datasource: Thanos
                proxmox:
                    proxmox-otel:
                        gnetId: 23855
                        revision: 1
                        datasource:
                            - name: DS_PROMETHEUS
                              value: "Thanos"
                external-dns:
                    external-dns:
                        gnetId: 15038
                        revision: 3
                        datasource: Thanos

        alertmanager:
            config:
                route:
                    receiver: "Axiom Notifications"
                    group_by: ["alertname", "cluster", "service"]
                    group_wait: 10s
                    group_interval: 10s
                    repeat_interval: 12h
                    routes:
                        # TODO: Should be routed to deadmanssnitch
                        - matchers:
                              - alertname = "Watchdog"
                          receiver: null
                        - matchers:
                              - alertname = "etcdInsufficientMembers"
                          receiver: null
                        - matchers:
                              # bad but not terrible
                              - alertname = "KubeAPIErrorBudgetBurn"
                          receiver: null
                receivers:
                    - name: "Axiom Notifications"
                      telegram_configs:
                          - chat_id: -1003735046157
                            bot_token_file: /secrets/axiom-telegrambot-alertmanager/token
            alertmanagerSpec:
                volumes:
                    - name: axiom-telegrambot-alertmanager
                      secret:
                          secretName: axiom-telegrambot-alertmanager
                          optional: false

                volumeMounts:
                    - name: axiom-telegrambot-alertmanager
                      mountPath: /secrets/axiom-telegrambot-alertmanager
                      readOnly: true
